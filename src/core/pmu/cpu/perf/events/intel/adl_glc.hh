#pragma once
#include <cstdint>
#define INTEL_X86_EDGE_BIT	18
#define INTEL_X86_ANY_BIT	21
#define INTEL_X86_INV_BIT	23
#define INTEL_X86_CMASK_BIT 24
#define INTEL_X86_MOD_EDGE	(1 << INTEL_X86_EDGE_BIT)
#define INTEL_X86_MOD_ANY	(1 << INTEL_X86_ANY_BIT)
#define INTEL_X86_MOD_INV	(1 << INTEL_X86_INV_BIT)
namespace optkit::intel::adl_glc{
	enum adl_glc : uint64_t {
		ARITH = 0x00b0, // This event is deprecated. Refer to new event ARITH.FPDIV_ACTIVE
		ARITH__MASK__ADL_GLC_ARITH__DIVIDER_ACTIVE = 0x0900ull | (0x1 << INTEL_X86_CMASK_BIT), // This event is deprecated. Refer to new event ARITH.DIV_ACTIVE
		ARITH__MASK__ADL_GLC_ARITH__DIV_ACTIVE = 0x0900ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when divide unit is busy executing divide or square root operations.
		ARITH__MASK__ADL_GLC_ARITH__FPDIV_ACTIVE = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // TBD
		ARITH__MASK__ADL_GLC_ARITH__FP_DIVIDER_ACTIVE = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // This event is deprecated. Refer to new event ARITH.FPDIV_ACTIVE
		ARITH__MASK__ADL_GLC_ARITH__IDIV_ACTIVE = 0x0800ull | (0x1 << INTEL_X86_CMASK_BIT), // This event counts the cycles the integer divider is busy.
		ARITH__MASK__ADL_GLC_ARITH__INT_DIVIDER_ACTIVE = 0x0800ull | (0x1 << INTEL_X86_CMASK_BIT), // This event is deprecated. Refer to new event ARITH.IDIV_ACTIVE
		ASSISTS = 0x00c1, // Counts all microcode FP assists.
		ASSISTS__MASK__ADL_GLC_ASSISTS__ANY = 0x1b00ull, // Number of occurrences where a microcode assist is invoked by hardware.
		ASSISTS__MASK__ADL_GLC_ASSISTS__FP = 0x0200ull, // Counts all microcode FP assists.
		ASSISTS__MASK__ADL_GLC_ASSISTS__HARDWARE = 0x0400ull, // TBD
		ASSISTS__MASK__ADL_GLC_ASSISTS__PAGE_FAULT = 0x0800ull, // TBD
		ASSISTS__MASK__ADL_GLC_ASSISTS__SSE_AVX_MIX = 0x1000ull, // TBD
		BACLEARS = 0x0060, // Clears due to Unknown Branches.
		BACLEARS__MASK__ADL_GLC_BACLEARS__ANY = 0x0100ull, // Clears due to Unknown Branches.
		BR_INST_RETIRED = 0x00c4, // All branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__ALL_BRANCHES = 0x0000ull, // All branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__COND = 0x1100ull, // Conditional branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__COND_NTAKEN = 0x1000ull, // Not taken branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__COND_TAKEN = 0x0100ull, // Taken conditional branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__FAR_BRANCH = 0x4000ull, // Far branch instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__INDIRECT = 0x8000ull, // Indirect near branch instructions retired (excluding returns)
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__NEAR_CALL = 0x0200ull, // Direct and indirect near call instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__NEAR_RETURN = 0x0800ull, // Return instructions retired.
		BR_INST_RETIRED__MASK__ADL_GLC_BR_INST_RETIRED__NEAR_TAKEN = 0x2000ull, // Taken branch instructions retired.
		BR_MISP_RETIRED = 0x00c5, // All mispredicted branch instructions retired.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__ALL_BRANCHES = 0x0000ull, // All mispredicted branch instructions retired.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__COND = 0x1100ull, // Mispredicted conditional branch instructions retired.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__COND_NTAKEN = 0x1000ull, // Mispredicted non-taken conditional branch instructions retired.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__COND_TAKEN = 0x0100ull, // number of branch instructions retired that were mispredicted and taken.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__INDIRECT = 0x8000ull, // Miss-predicted near indirect branch instructions retired (excluding returns)
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__INDIRECT_CALL = 0x0200ull, // Mispredicted indirect CALL retired.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__NEAR_TAKEN = 0x2000ull, // Number of near branch instructions retired that were mispredicted and taken.
		BR_MISP_RETIRED__MASK__ADL_GLC_BR_MISP_RETIRED__RET = 0x0800ull, // This event counts the number of mispredicted ret instructions retired. Non PEBS
		CORE_POWER = 0x0028, // TBD
		CORE_POWER__MASK__ADL_GLC_CORE_POWER__LICENSE_1 = 0x0200ull, // TBD
		CORE_POWER__MASK__ADL_GLC_CORE_POWER__LICENSE_2 = 0x0400ull, // TBD
		CORE_POWER__MASK__ADL_GLC_CORE_POWER__LICENSE_3 = 0x0800ull, // TBD
		CPU_CLK_UNHALTED = 0x003c, // Core cycles when the thread is not in halt state
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__C01 = 0x1000ull, // Core clocks when the thread is in the C0.1 light-weight slower wakeup time but more power saving optimized state.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__C02 = 0x2000ull, // Core clocks when the thread is in the C0.2 light-weight faster wakeup time but less power saving optimized state.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__C0_WAIT = 0x7000ull, // Core clocks when the thread is in the C0.1 or C0.2 or running a PAUSE in C0 ACPI state.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__DISTRIBUTED = 0x0200ull, // Cycle counts are evenly distributed between active threads in the Core.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__ONE_THREAD_ACTIVE = 0x0200ull, // Core crystal clock cycles when this thread is unhalted and the other thread is halted.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__PAUSE = 0x4000ull, // TBD
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__PAUSE_INST = 0x4000ull | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // TBD
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__REF_DISTRIBUTED = 0x0800ull, // Core crystal clock cycles. Cycle counts are evenly distributed between active threads in the Core.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__REF_TSC = 0x0300ull, // Reference cycles when the core is not in halt state.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__REF_TSC_P = 0x0100ull, // Reference cycles when the core is not in halt state.
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__THREAD = 0x0200ull, // Core cycles when the thread is not in halt state
		CPU_CLK_UNHALTED__MASK__ADL_GLC_CPU_CLK_UNHALTED__THREAD_P = 0x0000ull, // Thread cycles when thread is not in halt state
		CYCLE_ACTIVITY = 0x00a3, // Cycles while L2 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__CYCLES_L1D_MISS = 0x0800ull | (0x8 << INTEL_X86_CMASK_BIT), // Cycles while L1 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__CYCLES_L2_MISS = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles while L2 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__CYCLES_MEM_ANY = 0x1000ull | (0x10 << INTEL_X86_CMASK_BIT), // Cycles while memory subsystem has an outstanding load.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__STALLS_L1D_MISS = 0x0c00ull | (0xc << INTEL_X86_CMASK_BIT), // Execution stalls while L1 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__STALLS_L2_MISS = 0x0500ull | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls while L2 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__STALLS_L3_MISS = 0x0600ull | (0x6 << INTEL_X86_CMASK_BIT), // Execution stalls while L3 cache miss demand load is outstanding.
		CYCLE_ACTIVITY__MASK__ADL_GLC_CYCLE_ACTIVITY__STALLS_TOTAL = 0x0400ull | (0x4 << INTEL_X86_CMASK_BIT), // Total execution stalls.
		DECODE = 0x0087, // Stalls caused by changing prefix length of the instruction.
		DECODE__MASK__ADL_GLC_DECODE__LCP = 0x0100ull, // Stalls caused by changing prefix length of the instruction.
		DECODE__MASK__ADL_GLC_DECODE__MS_BUSY = 0x0200ull, // Cycles the Microcode Sequencer is busy.
		DSB2MITE_SWITCHES = 0x0061, // DSB-to-MITE switch true penalty cycles.
		DSB2MITE_SWITCHES__MASK__ADL_GLC_DSB2MITE_SWITCHES__PENALTY_CYCLES = 0x0200ull, // DSB-to-MITE switch true penalty cycles.
		DTLB_LOAD_MISSES = 0x0012, // Page walks completed due to a demand data load to a 4K page.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__STLB_HIT = 0x2000ull, // Loads that miss the DTLB and hit the STLB.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_ACTIVE = 0x1000ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when at least one PMH is busy with a page walk for a demand load.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_COMPLETED = 0x0e00ull, // Load miss in all TLB levels causes a page walk that completes. (All page sizes)
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_COMPLETED_1G = 0x0800ull, // Page walks completed due to a demand data load to a 1G page.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_COMPLETED_2M_4M = 0x0400ull, // Page walks completed due to a demand data load to a 2M/4M page.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_COMPLETED_4K = 0x0200ull, // Page walks completed due to a demand data load to a 4K page.
		DTLB_LOAD_MISSES__MASK__ADL_GLC_DTLB_LOAD_MISSES__WALK_PENDING = 0x1000ull, // Number of page walks outstanding for a demand load in the PMH each cycle.
		DTLB_STORE_MISSES = 0x0013, // Page walks completed due to a demand data store to a 4K page.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__STLB_HIT = 0x2000ull, // Stores that miss the DTLB and hit the STLB.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_ACTIVE = 0x1000ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when at least one PMH is busy with a page walk for a store.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_COMPLETED = 0x0e00ull, // Store misses in all TLB levels causes a page walk that completes. (All page sizes)
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_COMPLETED_1G = 0x0800ull, // Page walks completed due to a demand data store to a 1G page.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_COMPLETED_2M_4M = 0x0400ull, // Page walks completed due to a demand data store to a 2M/4M page.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_COMPLETED_4K = 0x0200ull, // Page walks completed due to a demand data store to a 4K page.
		DTLB_STORE_MISSES__MASK__ADL_GLC_DTLB_STORE_MISSES__WALK_PENDING = 0x1000ull, // Number of page walks outstanding for a store in the PMH each cycle.
		EXE_ACTIVITY = 0x00a6, // Cycles total of 1 uop is executed on all ports and Reservation Station was not empty.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__1_PORTS_UTIL = 0x0200ull, // Cycles total of 1 uop is executed on all ports and Reservation Station was not empty.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__2_PORTS_UTIL = 0x0400ull, // Cycles total of 2 uops are executed on all ports and Reservation Station was not empty.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__3_PORTS_UTIL = 0x0800ull, // Cycles total of 3 uops are executed on all ports and Reservation Station was not empty.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__4_PORTS_UTIL = 0x1000ull, // Cycles total of 4 uops are executed on all ports and Reservation Station was not empty.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__BOUND_ON_LOADS = 0x2100ull | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls while memory subsystem has an outstanding load.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__BOUND_ON_STORES = 0x4000ull | (0x2 << INTEL_X86_CMASK_BIT), // Cycles where the Store Buffer was full and no loads caused an execution stall.
		EXE_ACTIVITY__MASK__ADL_GLC_EXE_ACTIVITY__EXE_BOUND_0_PORTS = 0x8000ull, // Cycles no uop executed while RS was not empty
		FP_ARITH_DISPATCHED = 0x00b3, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__PORT_0 = 0x0100ull, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__PORT_1 = 0x0200ull, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__PORT_5 = 0x0400ull, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__V0 = 0x0100ull, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__V1 = 0x0200ull, // TBD
		FP_ARITH_DISPATCHED__MASK__ADL_GLC_FP_ARITH_DISPATCHED__V5 = 0x0400ull, // TBD
		FP_ARITH_INST_RETIRED = 0x00c7, // Counts number of SSE/AVX computational scalar double precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 1 computational operation. Applies to SSE* and AVX* scalar double precision floating-point instructions
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__128B_PACKED_DOUBLE = 0x0400ull, // Counts number of SSE/AVX computational 128-bit packed double precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 2 computation operations
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__128B_PACKED_SINGLE = 0x0800ull, // Counts number of SSE/AVX computational 128-bit packed single precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 4 computation operations
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__256B_PACKED_DOUBLE = 0x1000ull, // Counts number of SSE/AVX computational 256-bit packed double precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 4 computation operations
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__256B_PACKED_SINGLE = 0x2000ull, // Counts number of SSE/AVX computational 256-bit packed single precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 8 computation operations
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__4_FLOPS = 0x1800ull, // Counts number of SSE/AVX computational 128-bit packed single and 256-bit packed double precision FP instructions retired; some instructions will count twice as noted below.  Each count represents 2 or/and 4 computation operations
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__SCALAR = 0x0300ull, // Counts number of SSE/AVX computational scalar floating-point instructions retired; some instructions will count twice as noted below.  Applies to SSE* and AVX* scalar
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__SCALAR_DOUBLE = 0x0100ull, // Counts number of SSE/AVX computational scalar double precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 1 computational operation. Applies to SSE* and AVX* scalar double precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they perform 2 calculations per element.
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__SCALAR_SINGLE = 0x0200ull, // Counts number of SSE/AVX computational scalar single precision floating-point instructions retired; some instructions will count twice as noted below.  Each count represents 1 computational operation. Applies to SSE* and AVX* scalar single precision floating-point instructions: ADD SUB MUL DIV MIN MAX SQRT RSQRT RCP FM(N)ADD/SUB. FM(N)ADD/SUB instructions count twice as they perform 2 calculations per element.
		FP_ARITH_INST_RETIRED__MASK__ADL_GLC_FP_ARITH_INST_RETIRED__VECTOR = 0xfc00ull, // Number of any Vector retired FP arithmetic instructions
		FRONTEND_RETIRED = 0x01c6, // Retired Instructions who experienced a critical DSB miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__ANY_DSB_MISS = 0x0100ull, // Retired Instructions who experienced DSB miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__DSB_MISS = 0x1100ull, // Retired Instructions who experienced a critical DSB miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__ITLB_MISS = 0x1400ull, // Retired Instructions who experienced iTLB true miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__L1I_MISS = 0x1200ull, // Retired Instructions who experienced Instruction L1 Cache true miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__L2_MISS = 0x1300ull, // Retired Instructions who experienced Instruction L2 Cache true miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_1 = 0x60010600ull, // Retired instructions after front-end starvation of at least 1 cycle
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_128 = 0x60800600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 128 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_16 = 0x60100600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 16 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_2 = 0x60020600ull, // Retired instructions after front-end starvation of at least 2 cycles
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_256 = 0x61000600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 256 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_2_BUBBLES_GE_1 = 0x10020600ull, // Retired instructions that are fetched after an interval where the front-end had at least 1 bubble-slot for a period of 2 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_32 = 0x60200600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 32 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_4 = 0x60040600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 4 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_512 = 0x62000600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 512 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_64 = 0x60400600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 64 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__LATENCY_GE_8 = 0x60080600ull, // Retired instructions that are fetched after an interval where the front-end delivered no uops for a period of 8 cycles which was not interrupted by a back-end stall.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__MS_FLOWS = 0x0800ull, // TBD
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__STLB_MISS = 0x1500ull, // Retired Instructions who experienced STLB (2nd level TLB) true miss.
		FRONTEND_RETIRED__MASK__ADL_GLC_FRONTEND_RETIRED__UNKNOWN_BRANCH = 0x1700ull, // TBD
		ICACHE_DATA = 0x0080, // Cycles where a code fetch is stalled due to L1 instruction cache miss.
		ICACHE_DATA__MASK__ADL_GLC_ICACHE_DATA__STALLS = 0x0400ull, // Cycles where a code fetch is stalled due to L1 instruction cache miss.
		ICACHE_TAG = 0x0083, // Cycles where a code fetch is stalled due to L1 instruction cache tag miss.
		ICACHE_TAG__MASK__ADL_GLC_ICACHE_TAG__STALLS = 0x0400ull, // Cycles where a code fetch is stalled due to L1 instruction cache tag miss.
		IDQ = 0x0079, // Cycles MITE is delivering any Uop
		IDQ__MASK__ADL_GLC_IDQ__DSB_CYCLES_ANY = 0x0800ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles Decode Stream Buffer (DSB) is delivering any Uop
		IDQ__MASK__ADL_GLC_IDQ__DSB_CYCLES_OK = 0x0800ull | (0x6 << INTEL_X86_CMASK_BIT), // Cycles DSB is delivering optimal number of Uops
		IDQ__MASK__ADL_GLC_IDQ__DSB_UOPS = 0x0800ull, // Uops delivered to Instruction Decode Queue (IDQ) from the Decode Stream Buffer (DSB) path
		IDQ__MASK__ADL_GLC_IDQ__MITE_CYCLES_ANY = 0x0400ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering any Uop
		IDQ__MASK__ADL_GLC_IDQ__MITE_CYCLES_OK = 0x0400ull | (0x6 << INTEL_X86_CMASK_BIT), // Cycles MITE is delivering optimal number of Uops
		IDQ__MASK__ADL_GLC_IDQ__MITE_UOPS = 0x0400ull, // Uops delivered to Instruction Decode Queue (IDQ) from MITE path
		IDQ__MASK__ADL_GLC_IDQ__MS_CYCLES_ANY = 0x2000ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when uops are being delivered to IDQ while MS is busy
		IDQ__MASK__ADL_GLC_IDQ__MS_SWITCHES = 0x2000ull | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // Number of switches from DSB or MITE to the MS
		IDQ__MASK__ADL_GLC_IDQ__MS_UOPS = 0x2000ull, // Uops delivered to IDQ while MS is busy
		IDQ_BUBBLES = 0x009c, // Uops not delivered by IDQ when backend of the machine is not stalled
		IDQ_BUBBLES__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CORE = 0x0100ull, // Uops not delivered by IDQ when backend of the machine is not stalled [This event is alias to IDQ_BUBBLES.CORE]
		IDQ_BUBBLES__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x0100ull | (0x6 << INTEL_X86_CMASK_BIT), // Cycles when no uops are not delivered by the IDQ when backend of the machine is not stalled [This event is alias to IDQ_BUBBLES.CYCLES_0_UOPS_DELIV.CORE]
		IDQ_BUBBLES__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x0100ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when optimal number of uops was delivered to the back-end when the back-end is not stalled [This event is alias to IDQ_BUBBLES.CYCLES_FE_WAS_OK]
		IDQ_UOPS_NOT_DELIVERED = 0x009c, // Uops not delivered by IDQ when backend of the machine is not stalled [This event is alias to IDQ_BUBBLES.CORE]
		IDQ_UOPS_NOT_DELIVERED__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CORE = 0x0100ull, // Uops not delivered by IDQ when backend of the machine is not stalled [This event is alias to IDQ_BUBBLES.CORE]
		IDQ_UOPS_NOT_DELIVERED__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CYCLES_0_UOPS_DELIV_CORE = 0x0100ull | (0x6 << INTEL_X86_CMASK_BIT), // Cycles when no uops are not delivered by the IDQ when backend of the machine is not stalled [This event is alias to IDQ_BUBBLES.CYCLES_0_UOPS_DELIV.CORE]
		IDQ_UOPS_NOT_DELIVERED__MASK__ADL_GLC_IDQ_UOPS_NOT_DELIVERED__CYCLES_FE_WAS_OK = 0x0100ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when optimal number of uops was delivered to the back-end when the back-end is not stalled [This event is alias to IDQ_BUBBLES.CYCLES_FE_WAS_OK]
		INST_DECODED = 0x0075, // Instruction decoders utilized in a cycle
		INST_DECODED__MASK__ADL_GLC_INST_DECODED__DECODERS = 0x0100ull, // Instruction decoders utilized in a cycle
		INST_RETIRED = 0x00c0, // Number of instructions retired
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__ANY = 0x0100ull, // Number of instructions retired. Fixed Counter - architectural event
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__ANY_P = 0x0000ull, // Number of instructions retired. General Counter - architectural event
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__MACRO_FUSED = 0x1000ull, // TBD
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__NOP = 0x0200ull, // Retired NOP instructions.
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__PREC_DIST = 0x0100ull, // Precise instruction retired with PEBS precise-distribution
		INST_RETIRED__MASK__ADL_GLC_INST_RETIRED__REP_ITERATION = 0x0800ull, // Iterations of Repeat string retired instructions.
		INSTRUCTION_RETIRED = 0xc0, // Number of instructions at retirement
		INSTRUCTIONS_RETIRED = 0xc0, // Number of instructions at retirement
		INT_MISC = 0x00ad, // Core cycles the allocator was stalled due to recovery from earlier clear event for this thread
		INT_MISC__MASK__ADL_GLC_INT_MISC__CLEARS_COUNT = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // Clears speculative count
		INT_MISC__MASK__ADL_GLC_INT_MISC__CLEAR_RESTEER_CYCLES = 0x8000ull, // Counts cycles after recovery from a branch misprediction or machine clear till the first uop is issued from the resteered path.
		INT_MISC__MASK__ADL_GLC_INT_MISC__RECOVERY_CYCLES = 0x0100ull, // Core cycles the allocator was stalled due to recovery from earlier clear event for this thread
		INT_MISC__MASK__ADL_GLC_INT_MISC__UNKNOWN_BRANCH_CYCLES = 0x0700ull, // Bubble cycles of BAClear (Unknown Branch).
		INT_MISC__MASK__ADL_GLC_INT_MISC__UOP_DROPPING = 0x1000ull, // TMA slots where uops got dropped
		INT_VEC_RETIRED = 0x00e7, // integer ADD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__128BIT = 0x1300ull, // TBD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__256BIT = 0xac00ull, // TBD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__ADD_128 = 0x0300ull, // integer ADD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__ADD_256 = 0x0c00ull, // integer ADD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__MUL_256 = 0x8000ull, // TBD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__SHUFFLES = 0x4000ull, // TBD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__VNNI_128 = 0x1000ull, // TBD
		INT_VEC_RETIRED__MASK__ADL_GLC_INT_VEC_RETIRED__VNNI_256 = 0x2000ull, // TBD
		ITLB_MISSES = 0x0011, // Code miss in all TLB levels causes a page walk that completes. (4K)
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__STLB_HIT = 0x2000ull, // Instruction fetch requests that miss the ITLB and hit the STLB.
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__WALK_ACTIVE = 0x1000ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles when at least one PMH is busy with a page walk for code (instruction fetch) request.
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__WALK_COMPLETED = 0x0e00ull, // Code miss in all TLB levels causes a page walk that completes. (All page sizes)
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__WALK_COMPLETED_2M_4M = 0x0400ull, // Code miss in all TLB levels causes a page walk that completes. (2M/4M)
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__WALK_COMPLETED_4K = 0x0200ull, // Code miss in all TLB levels causes a page walk that completes. (4K)
		ITLB_MISSES__MASK__ADL_GLC_ITLB_MISSES__WALK_PENDING = 0x1000ull, // Number of page walks outstanding for an outstanding code request in the PMH each cycle.
		L1D = 0x0051, // Counts the number of cache lines replaced in L1 data cache.
		L1D__MASK__ADL_GLC_L1D__HWPF_MISS = 0x2000ull, // TBD
		L1D__MASK__ADL_GLC_L1D__REPLACEMENT = 0x0100ull, // Counts the number of cache lines replaced in L1 data cache.
		L1D_PEND_MISS = 0x0048, // Number of L1D misses that are outstanding
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__FB_FULL = 0x0200ull, // Number of cycles a demand request has waited due to L1D Fill Buffer (FB) unavailability.
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__FB_FULL_PERIODS = 0x0200ull | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // Number of phases a demand request has waited due to L1D Fill Buffer (FB) unavailability.
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__L2_STALL = 0x0400ull, // This event is deprecated. Refer to new event L1D_PEND_MISS.L2_STALLS
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__L2_STALLS = 0x0400ull, // Number of cycles a demand request has waited due to L1D due to lack of L2 resources.
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__PENDING = 0x0100ull, // Number of L1D misses that are outstanding
		L1D_PEND_MISS__MASK__ADL_GLC_L1D_PEND_MISS__PENDING_CYCLES = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with L1D load Misses outstanding.
		L2_LINES_IN = 0x0025, // L2 cache lines filling L2
		L2_LINES_IN__MASK__ADL_GLC_L2_LINES_IN__ALL = 0x1f00ull, // L2 cache lines filling L2
		L2_LINES_OUT = 0x0026, // Cache lines that have been L2 hardware prefetched but not used by demand accesses
		L2_LINES_OUT__MASK__ADL_GLC_L2_LINES_OUT__USELESS_HWPF = 0x0400ull, // Cache lines that have been L2 hardware prefetched but not used by demand accesses
		L2_REQUEST = 0x0024, // Demand Data Read miss L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__ALL_CODE_RD = 0xe400ull, // L2 code requests
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__ALL_DEMAND_DATA_RD = 0xe100ull, // Demand Data Read access L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__ALL_DEMAND_MISS = 0x2700ull, // Demand requests that miss L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__ALL_HWPF = 0xf000ull, // TBD
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__ALL_RFO = 0xe200ull, // RFO requests to L2 cache.
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__CODE_RD_HIT = 0xc400ull, // L2 cache hits when fetching instructions
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__CODE_RD_MISS = 0x2400ull, // L2 cache misses when fetching instructions
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__DEMAND_DATA_RD_HIT = 0xc100ull, // Demand Data Read requests that hit L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__DEMAND_DATA_RD_MISS = 0x2100ull, // Demand Data Read miss L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__HWPF_MISS = 0x3000ull, // TBD
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__MISS = 0x3f00ull, // Read requests with true-miss in L2 cache. [This event is alias to L2_REQUEST.MISS]
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__REFERENCES = 0xff00ull, // All accesses to L2 cache [This event is alias to L2_REQUEST.ALL]
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__RFO_HIT = 0xc200ull, // RFO requests that hit L2 cache.
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__RFO_MISS = 0x2200ull, // RFO requests that miss L2 cache
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__SWPF_HIT = 0xc800ull, // SW prefetch requests that hit L2 cache.
		L2_REQUEST__MASK__ADL_GLC_L2_RQSTS__SWPF_MISS = 0x2800ull, // SW prefetch requests that miss L2 cache.
		L2_RQSTS = 0x0024, // Demand Data Read miss L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__ALL_CODE_RD = 0xe400ull, // L2 code requests
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__ALL_DEMAND_DATA_RD = 0xe100ull, // Demand Data Read access L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__ALL_DEMAND_MISS = 0x2700ull, // Demand requests that miss L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__ALL_HWPF = 0xf000ull, // TBD
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__ALL_RFO = 0xe200ull, // RFO requests to L2 cache.
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__CODE_RD_HIT = 0xc400ull, // L2 cache hits when fetching instructions
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__CODE_RD_MISS = 0x2400ull, // L2 cache misses when fetching instructions
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__DEMAND_DATA_RD_HIT = 0xc100ull, // Demand Data Read requests that hit L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__DEMAND_DATA_RD_MISS = 0x2100ull, // Demand Data Read miss L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__HWPF_MISS = 0x3000ull, // TBD
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__MISS = 0x3f00ull, // Read requests with true-miss in L2 cache. [This event is alias to L2_REQUEST.MISS]
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__REFERENCES = 0xff00ull, // All accesses to L2 cache [This event is alias to L2_REQUEST.ALL]
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__RFO_HIT = 0xc200ull, // RFO requests that hit L2 cache.
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__RFO_MISS = 0x2200ull, // RFO requests that miss L2 cache
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__SWPF_HIT = 0xc800ull, // SW prefetch requests that hit L2 cache.
		L2_RQSTS__MASK__ADL_GLC_L2_RQSTS__SWPF_MISS = 0x2800ull, // SW prefetch requests that miss L2 cache.
		LD_BLOCKS = 0x0003, // False dependencies in MOB due to partial compare on address.
		LD_BLOCKS__MASK__ADL_GLC_LD_BLOCKS__ADDRESS_ALIAS = 0x0400ull, // False dependencies in MOB due to partial compare on address.
		LD_BLOCKS__MASK__ADL_GLC_LD_BLOCKS__NO_SR = 0x8800ull, // The number of times that split load operations are temporarily blocked because all resources for handling the split accesses are in use.
		LD_BLOCKS__MASK__ADL_GLC_LD_BLOCKS__STORE_FORWARD = 0x8200ull, // Loads blocked due to overlapping with a preceding store that cannot be forwarded.
		LOAD_HIT_PREFETCH = 0x004c, // Counts the number of demand load dispatches that hit L1D fill buffer (FB) allocated for software prefetch.
		LOAD_HIT_PREFETCH__MASK__ADL_GLC_LOAD_HIT_PREFETCH__SWPF = 0x0100ull, // Counts the number of demand load dispatches that hit L1D fill buffer (FB) allocated for software prefetch.
		LONGEST_LAT_CACHE = 0x002e, // Core-originated cacheable requests that missed L3  (Except hardware prefetches to the L3)
		LONGEST_LAT_CACHE__MASK__ADL_GLC_LONGEST_LAT_CACHE__MISS = 0x4100ull, // Core-originated cacheable requests that missed L3  (Except hardware prefetches to the L3)
		LONGEST_LAT_CACHE__MASK__ADL_GLC_LONGEST_LAT_CACHE__REFERENCE = 0x4f00ull, // Core-originated cacheable requests that refer to L3 (Except hardware prefetches to the L3)
		LSD = 0x00a8, // Cycles Uops delivered by the LSD
		LSD__MASK__ADL_GLC_LSD__CYCLES_ACTIVE = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles Uops delivered by the LSD
		LSD__MASK__ADL_GLC_LSD__CYCLES_OK = 0x0100ull | (0x6 << INTEL_X86_CMASK_BIT), // Cycles optimal number of Uops delivered by the LSD
		LSD__MASK__ADL_GLC_LSD__UOPS = 0x0100ull, // Number of Uops delivered by the LSD.
		MACHINE_CLEARS = 0x00c3, // Number of machine clears (nukes) of any type.
		MACHINE_CLEARS__MASK__ADL_GLC_MACHINE_CLEARS__COUNT = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // Number of machine clears (nukes) of any type.
		MACHINE_CLEARS__MASK__ADL_GLC_MACHINE_CLEARS__MEMORY_ORDERING = 0x0200ull, // Number of machine clears due to memory ordering conflicts.
		MACHINE_CLEARS__MASK__ADL_GLC_MACHINE_CLEARS__SMC = 0x0400ull, // Self-modifying code (SMC) detected.
		MEMORY_ACTIVITY = 0x0047, // Cycles while L1 cache miss demand load is outstanding.
		MEMORY_ACTIVITY__MASK__ADL_GLC_MEMORY_ACTIVITY__CYCLES_L1D_MISS = 0x0200ull | (0x2 << INTEL_X86_CMASK_BIT), // Cycles while L1 cache miss demand load is outstanding.
		MEMORY_ACTIVITY__MASK__ADL_GLC_MEMORY_ACTIVITY__STALLS_L1D_MISS = 0x0300ull | (0x3 << INTEL_X86_CMASK_BIT), // Execution stalls while L1 cache miss demand load is outstanding.
		MEMORY_ACTIVITY__MASK__ADL_GLC_MEMORY_ACTIVITY__STALLS_L2_MISS = 0x0500ull | (0x5 << INTEL_X86_CMASK_BIT), // Execution stalls while L2 cache miss demand cacheable load request is outstanding.
		MEMORY_ACTIVITY__MASK__ADL_GLC_MEMORY_ACTIVITY__STALLS_L3_MISS = 0x0900ull | (0x9 << INTEL_X86_CMASK_BIT), // Execution stalls while L3 cache miss demand cacheable load request is outstanding.
		MEM_INST_RETIRED = 0x00d0, // Retired load instructions that miss the STLB.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__ALL_LOADS = 0x8100ull, // Retired load instructions.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__ALL_STORES = 0x8200ull, // Retired store instructions.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__ANY = 0x8300ull, // All retired memory instructions.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__LOCK_LOADS = 0x2100ull, // Retired load instructions with locked access.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__SPLIT_LOADS = 0x4100ull, // Retired load instructions that split across a cacheline boundary.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__SPLIT_STORES = 0x4200ull, // Retired store instructions that split across a cacheline boundary.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__STLB_MISS_LOADS = 0x1100ull, // Retired load instructions that miss the STLB.
		MEM_INST_RETIRED__MASK__ADL_GLC_MEM_INST_RETIRED__STLB_MISS_STORES = 0x1200ull, // Retired store instructions that miss the STLB.
		MEM_LOAD_COMPLETED = 0x0043, // Completed demand load uops that miss the L1 d-cache.
		MEM_LOAD_COMPLETED__MASK__ADL_GLC_MEM_LOAD_COMPLETED__L1_MISS_ANY = 0xfd00ull, // Completed demand load uops that miss the L1 d-cache.
		MEM_LOAD_L3_HIT_RETIRED = 0x00d2, // Retired load instructions whose data sources were L3 hit and cross-core snoop missed in on-pkg core cache.
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_FWD = 0x0400ull, // Retired load instructions whose data sources were HitM responses from shared L3
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_HIT = 0x0200ull, // Retired load instructions whose data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_HITM = 0x0400ull, // Retired load instructions whose data sources were HitM responses from shared L3
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_MISS = 0x0100ull, // Retired load instructions whose data sources were L3 hit and cross-core snoop missed in on-pkg core cache.
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_NONE = 0x0800ull, // Retired load instructions whose data sources were hits in L3 without snoops required
		MEM_LOAD_L3_HIT_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_HIT_RETIRED__XSNP_NO_FWD = 0x0200ull, // Retired load instructions whose data sources were L3 and cross-core snoop hits in on-pkg core cache
		MEM_LOAD_L3_MISS_RETIRED = 0x00d3, // Retired load instructions which data sources missed L3 but serviced from local dram
		MEM_LOAD_L3_MISS_RETIRED__MASK__ADL_GLC_MEM_LOAD_L3_MISS_RETIRED__LOCAL_DRAM = 0x0100ull, // Retired load instructions which data sources missed L3 but serviced from local dram
		MEM_LOAD_MISC_RETIRED = 0x00d4, // Retired instructions with at least 1 uncacheable load or lock.
		MEM_LOAD_MISC_RETIRED__MASK__ADL_GLC_MEM_LOAD_MISC_RETIRED__UC = 0x0400ull, // Retired instructions with at least 1 uncacheable load or lock.
		MEM_LOAD_RETIRED = 0x00d1, // Retired load instructions with L1 cache hits as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__FB_HIT = 0x4000ull, // Number of completed demand load requests that missed the L1
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L1_HIT = 0x0100ull, // Retired load instructions with L1 cache hits as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L1_MISS = 0x0800ull, // Retired load instructions missed L1 cache as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L2_HIT = 0x0200ull, // Retired load instructions with L2 cache hits as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L2_MISS = 0x1000ull, // Retired load instructions missed L2 cache as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L3_HIT = 0x0400ull, // Retired load instructions with L3 cache hits as data sources
		MEM_LOAD_RETIRED__MASK__ADL_GLC_MEM_LOAD_RETIRED__L3_MISS = 0x2000ull, // Retired load instructions missed L3 cache as data sources
		MEM_STORE_RETIRED = 0x0044, // TBD
		MEM_STORE_RETIRED__MASK__ADL_GLC_MEM_STORE_RETIRED__L2_HIT = 0x0100ull, // TBD
		MEM_TRANS_RETIRED = 0x01cd, // 
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY = 0x100, // Memory load instructions retired above programmed clocks
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_128 = 0x8000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 128 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_16 = 0x1000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 16 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_256 = 0x10000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 256 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_32 = 0x2000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 32 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_4 = 0x0400ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 4 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_512 = 0x20000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 512 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_64 = 0x4000ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 64 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__LOAD_LATENCY_GT_8 = 0x0800ull, // Counts randomly selected loads when the latency from first dispatch to completion is greater than 8 cycles.
		MEM_TRANS_RETIRED__MASK__ADL_GLC_MEM_TRANS_RETIRED__STORE_SAMPLE = 0x0200ull, // Retired memory store access operations. A PDist event for PEBS Store Latency Facility.
		MEM_UOP_RETIRED = 0x00e5, // Retired memory uops for any access
		MEM_UOP_RETIRED__MASK__ADL_GLC_MEM_UOP_RETIRED__ANY = 0x0300ull, // Retired memory uops for any access
		MISC2_RETIRED = 0x00e0, // LFENCE instructions retired
		MISC2_RETIRED__MASK__ADL_GLC_MISC2_RETIRED__LFENCE = 0x2000ull, // LFENCE instructions retired
		MISC_RETIRED = 0x00cc, // Increments whenever there is an update to the LBR array.
		MISC_RETIRED__MASK__ADL_GLC_MISC_RETIRED__LBR_INSERTS = 0x2000ull, // Increments whenever there is an update to the LBR array.
		OCR0 = 0x012a, // Counts demand data reads that were not supplied by the L3 cache.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_ANY_RESPONSE = 0x1000100ull, // Counts demand data reads that have any type of response.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_DRAM = 0x18400000100ull, // Counts demand data reads that were supplied by DRAM.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_HIT_SNOOP_HITM = 0x10003c000100ull, // Counts demand data reads that resulted in a snoop hit in another cores caches
		OCR0__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_HIT_SNOOP_HIT_WITH_FWD = 0x8003c000100ull, // Counts demand data reads that resulted in a snoop hit in another cores caches which forwarded the unmodified data to the requesting core.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_MISS = 0x3fbfc0000100ull, // Counts demand data reads that were not supplied by the L3 cache.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_RFO_ANY_RESPONSE = 0x1000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that have any type of response.
		OCR0__MASK__ADL_GLC_OCR__DEMAND_RFO_L3_HIT_SNOOP_HITM = 0x10003c000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that resulted in a snoop hit in another cores caches
		OCR0__MASK__ADL_GLC_OCR__DEMAND_RFO_L3_MISS = 0x3fbfc0000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that were not supplied by the L3 cache.
		OCR0__MASK__ADL_GLC_OCR__STREAMING_WR_ANY_RESPONSE = 0x1080000ull, // Counts streaming stores that have any type of response.
		OCR1 = 0x012b, // Counts demand data reads that were not supplied by the L3 cache.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_ANY_RESPONSE = 0x1000100ull, // Counts demand data reads that have any type of response.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_DRAM = 0x18400000100ull, // Counts demand data reads that were supplied by DRAM.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_HIT_SNOOP_HITM = 0x10003c000100ull, // Counts demand data reads that resulted in a snoop hit in another cores caches
		OCR1__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_HIT_SNOOP_HIT_WITH_FWD = 0x8003c000100ull, // Counts demand data reads that resulted in a snoop hit in another cores caches which forwarded the unmodified data to the requesting core.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_DATA_RD_L3_MISS = 0x3fbfc0000100ull, // Counts demand data reads that were not supplied by the L3 cache.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_RFO_ANY_RESPONSE = 0x1000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that have any type of response.
		OCR1__MASK__ADL_GLC_OCR__DEMAND_RFO_L3_HIT_SNOOP_HITM = 0x10003c000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that resulted in a snoop hit in another cores caches
		OCR1__MASK__ADL_GLC_OCR__DEMAND_RFO_L3_MISS = 0x3fbfc0000200ull, // Counts demand read for ownership (RFO) requests and software prefetches for exclusive ownership (PREFETCHW) that were not supplied by the L3 cache.
		OCR1__MASK__ADL_GLC_OCR__STREAMING_WR_ANY_RESPONSE = 0x1080000ull, // Counts streaming stores that have any type of response.
		OFFCORE_REQUESTS = 0x0021, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__ADL_GLC_OFFCORE_REQUESTS__ALL_REQUESTS = 0x8000ull, // TBD
		OFFCORE_REQUESTS__MASK__ADL_GLC_OFFCORE_REQUESTS__DATA_RD = 0x0800ull, // Demand and prefetch data reads
		OFFCORE_REQUESTS__MASK__ADL_GLC_OFFCORE_REQUESTS__DEMAND_DATA_RD = 0x0100ull, // Demand Data Read requests sent to uncore
		OFFCORE_REQUESTS__MASK__ADL_GLC_OFFCORE_REQUESTS__L3_MISS_DEMAND_DATA_RD = 0x1000ull, // Counts demand data read requests that miss the L3 cache.
		OFFCORE_REQUESTS_OUTSTANDING = 0x0020, // Cycles where at least 1 outstanding demand data read request is pending.
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__ALL_DATA_RD = 0x0800ull, // This event is deprecated. Refer to new event OFFCORE_REQUESTS_OUTSTANDING.DATA_RD
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DATA_RD = 0x0800ull | (0x1 << INTEL_X86_CMASK_BIT), // TBD
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DEMAND_DATA_RD = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 outstanding demand data read request is pending.
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__CYCLES_WITH_DEMAND_RFO = 0x0400ull | (0x1 << INTEL_X86_CMASK_BIT), // For every cycle where the core is waiting on at least 1 outstanding Demand RFO request
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__DATA_RD = 0x0800ull, // TBD
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__DEMAND_DATA_RD = 0x0100ull, // For every cycle
		OFFCORE_REQUESTS_OUTSTANDING__MASK__ADL_GLC_OFFCORE_REQUESTS_OUTSTANDING__L3_MISS_DEMAND_DATA_RD = 0x1000ull, // For every cycle
		RESOURCE_STALLS = 0x00a2, // Counts cycles where the pipeline is stalled due to serializing operations.
		RESOURCE_STALLS__MASK__ADL_GLC_RESOURCE_STALLS__SB = 0x0800ull, // Cycles stalled due to no store buffers available. (not including draining form sync).
		RESOURCE_STALLS__MASK__ADL_GLC_RESOURCE_STALLS__SCOREBOARD = 0x0200ull, // Counts cycles where the pipeline is stalled due to serializing operations.
		RS = 0x00a5, // Cycles when Reservation Station (RS) is empty for the thread.
		RS__MASK__ADL_GLC_RS__EMPTY = 0x0700ull, // Cycles when Reservation Station (RS) is empty for the thread.
		RS__MASK__ADL_GLC_RS__EMPTY_COUNT = 0x0700ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT) | (0x1 << INTEL_X86_EDGE_BIT), // Counts end of periods where the Reservation Station (RS) was empty.
		SQ_MISC = 0x002c, // Counts bus locks
		SQ_MISC__MASK__ADL_GLC_SQ_MISC__BUS_LOCK = 0x1000ull, // Counts bus locks
		SW_PREFETCH_ACCESS = 0x0040, // Number of PREFETCHNTA instructions executed.
		SW_PREFETCH_ACCESS__MASK__ADL_GLC_SW_PREFETCH_ACCESS__NTA = 0x0100ull, // Number of PREFETCHNTA instructions executed.
		SW_PREFETCH_ACCESS__MASK__ADL_GLC_SW_PREFETCH_ACCESS__PREFETCHW = 0x0800ull, // Number of PREFETCHW instructions executed.
		SW_PREFETCH_ACCESS__MASK__ADL_GLC_SW_PREFETCH_ACCESS__T0 = 0x0200ull, // Number of PREFETCHT0 instructions executed.
		SW_PREFETCH_ACCESS__MASK__ADL_GLC_SW_PREFETCH_ACCESS__T1_T2 = 0x0400ull, // Number of PREFETCHT1 or PREFETCHT2 instructions executed.
		TOPDOWN = 0x0000, // Topdown events using PERF_METRICS support
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__BACKEND_BOUND_SLOTS = 0x8300ull, // TMA slots where no uops were being issued due to lack of back-end resources (Topdown L1)
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__BAD_SPEC_SLOTS = 0x8100ull, // TMA slots wasted due to incorrect speculations (Topdown L1)
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__BR_MISPREDICT_SLOTS = 0x8500ull, // TMA slots wasted due to incorrect speculation by branch mispredictions
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__MEMORY_BOUND_SLOTS = 0x8700ull, // TMA slots wasted due to memory accesses (TopdownL2)
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__RETIRING_SLOTS = 0x8000ull, // TMA slots where instructions are retiring (Topdown L1)
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__SLOTS = 0x0400ull, // TMA slots available for an unhalted logical processor. Fixed counter - architectural event
		TOPDOWN__MASK__ADL_GLC_TOPDOWN__SLOTS_P = 0x01a4ull, // TMA slots available for an unhalted logical processor. General counter - architectural event
		UNHALTED_CORE_CYCLES = 0x3c, // Count core clock cycles whenever the clock signal on the specific core is running (not halted)
		UNHALTED_REFERENCE_CYCLES = 0x0300, // Unhalted reference cycles
		UOPS_DECODED = 0x0076, // TBD
		UOPS_DECODED__MASK__ADL_GLC_UOPS_DECODED__DEC0_UOPS = 0x0100ull, // TBD
		UOPS_DISPATCHED = 0x00b2, // Uops executed on port 0
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_0 = 0x0100ull, // Uops executed on port 0
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_1 = 0x0200ull, // Uops executed on port 1
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_2_3_10 = 0x0400ull, // Uops executed on ports 2
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_4_9 = 0x1000ull, // Uops executed on ports 4 and 9
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_5_11 = 0x2000ull, // Uops executed on ports 5 and 11
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_6 = 0x4000ull, // Uops executed on port 6
		UOPS_DISPATCHED__MASK__ADL_GLC_UOPS_DISPATCHED__PORT_7_8 = 0x8000ull, // Uops executed on ports 7 and 8
		UOPS_EXECUTED = 0x00b1, // Cycles where at least 1 uop was executed per-thread
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CORE_CYCLES_GE_1 = 0x0200ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles at least 1 micro-op is executed from any thread on physical core.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CORE_CYCLES_GE_2 = 0x0200ull | (0x2 << INTEL_X86_CMASK_BIT), // Cycles at least 2 micro-op is executed from any thread on physical core.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CORE_CYCLES_GE_3 = 0x0200ull | (0x3 << INTEL_X86_CMASK_BIT), // Cycles at least 3 micro-op is executed from any thread on physical core.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CORE_CYCLES_GE_4 = 0x0200ull | (0x4 << INTEL_X86_CMASK_BIT), // Cycles at least 4 micro-op is executed from any thread on physical core.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CYCLES_GE_1 = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles where at least 1 uop was executed per-thread
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CYCLES_GE_2 = 0x0100ull | (0x2 << INTEL_X86_CMASK_BIT), // Cycles where at least 2 uops were executed per-thread
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CYCLES_GE_3 = 0x0100ull | (0x3 << INTEL_X86_CMASK_BIT), // Cycles where at least 3 uops were executed per-thread
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__CYCLES_GE_4 = 0x0100ull | (0x4 << INTEL_X86_CMASK_BIT), // Cycles where at least 4 uops were executed per-thread
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__STALLS = 0x0100ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // Counts number of cycles no uops were dispatched to be executed on this thread.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__STALL_CYCLES = 0x0100ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // This event is deprecated. Refer to new event UOPS_EXECUTED.STALLS
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__THREAD = 0x0100ull, // Counts the number of uops to be executed per-thread each cycle.
		UOPS_EXECUTED__MASK__ADL_GLC_UOPS_EXECUTED__X87 = 0x1000ull, // Counts the number of x87 uops dispatched.
		UOPS_ISSUED = 0x00ae, // Uops that RAT issues to RS
		UOPS_ISSUED__MASK__ADL_GLC_UOPS_ISSUED__ANY = 0x0100ull, // Uops that RAT issues to RS
		UOPS_RETIRED = 0x00c2, // Retired uops except the last uop of each instruction.
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__CYCLES = 0x0200ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles with retired uop(s).
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__HEAVY = 0x0100ull, // Retired uops except the last uop of each instruction.
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__MS = 0x0800ull, // TBD
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__SLOTS = 0x0200ull, // Retirement slots used.
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__STALLS = 0x0200ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // Cycles without actually retired uops.
		UOPS_RETIRED__MASK__ADL_GLC_UOPS_RETIRED__STALL_CYCLES = 0x0200ull | (0x1 << INTEL_X86_INV_BIT) | (0x1 << INTEL_X86_CMASK_BIT), // This event is deprecated. Refer to new event UOPS_RETIRED.STALLS
		XQ = 0x002d, // Cycles the uncore cannot take further requests
		XQ__MASK__ADL_GLC_XQ__FULL_CYCLES = 0x0100ull | (0x1 << INTEL_X86_CMASK_BIT), // Cycles the uncore cannot take further requests
		
	};
};

namespace adl_glc = optkit::intel::adl_glc;

#undef INTEL_X86_EDGE_BIT
#undef INTEL_X86_ANY_BIT
#undef INTEL_X86_INV_BIT
#undef INTEL_X86_CMASK_BIT
#undef INTEL_X86_MOD_EDGE
#undef INTEL_X86_MOD_ANY
#undef INTEL_X86_MOD_INV
